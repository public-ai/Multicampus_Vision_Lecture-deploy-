{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If You use in Colab, You Should run this script\n",
    "import os\n",
    "if (not os.path.exists(\"./SSD-object-detection\") and\n",
    "    not \"SSD-object-detection\" in os.getcwd()):\n",
    "    !git clone https://github.com/public-ai/SSD-object-detection.git\n",
    "    os.chdir(\"./SSD-object-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import DetectionDataset, draw_rectangle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ SSD Network Model 구성하기 \\]\n",
    "---\n",
    "---\n",
    "\n",
    "[SSD Network](https://arxiv.org/abs/1512.02325)은 한 번의 순전파로 사물을 포착하면서 사물을 분류할 수 있는 모델입니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Base Network 구성하기\n",
    "---\n",
    "\n",
    "SSD Network는 기본적으로 `VGG-Network` Style을 따릅니다. 여기에서는 좀 더 빠른 수렴을 위해, Batch Normalization을 추가하고, MaxPooling을 제거하도록 하겠습니다. Max Pooling은 사물의 위치에 무관하게 특징을 잘 잡을 수 있도록 도와주지만, 역으로 사물의 위치 정보를 훼손시키기 때문에 Detection과 같은 Task를 수행할 때에는 제거해주는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/A84mkR2.png)\n",
    "\n",
    "| layer name | num unit | filter size | stride | padding |\n",
    "|---|---|---|---|---|\n",
    "| `conv1_1` | 16 | (3,3) | (1,1) | same |\n",
    "| `norm1_1` | - | - | - | - |\n",
    "| `conv1_2` | 16 | (3,3) | (1,1) | same |\n",
    "| `norm1_2` | - | - | - | - |\n",
    "| `conv1_3` | 16 | (3,3) | (1,1) | same |\n",
    "| `norm1_3` | - | - | - | - |\n",
    "| `conv2_1` | 32 | (3,3) | (1,1) | same |\n",
    "| `norm2_1` | - | - | - | - |\n",
    "| `conv2_2` | 32 | (3,3) | (2,2) | same |\n",
    "| `norm2_2` | - | - | - | - |\n",
    "| `conv3_1` | 64 | (3,3) | (1,1) | same |\n",
    "| `norm3_1` | - | - | - | - |\n",
    "| `conv3_2` | 64 | (3,3) | (2,2) | same |\n",
    "| `norm3_2` | - | - | - | - |\n",
    "| `conv4_1` | 128 | (3,3) | (1,1) | same |\n",
    "| `norm4_1` | - | - | - | - |\n",
    "| `conv4_2` | 128 | (3,3) | (2,2) | same |\n",
    "| `norm4_2` | - | - | - | - |\n",
    "| `conv5_1` | 128 | (3,3) | (1,1) | same |\n",
    "| `norm5_1` | - | - | - | - |\n",
    "| `conv5_2` | 128 | (3,3) | (2,2) | same |\n",
    "| `norm5_2` | - | - | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때까지 보던 VGG Network와 ResNet과의 차이점은 마지막의 Layer에 있습니다. Head Network라고 불리는 저 부분은 Object Detection의 출력인 위치 정보와 클래스 정보로 구성됩니다. 노란색 Convolution Filter는 각 Feature Map의 점에서 어떤 사물이 있는지를 분류하고, 초록색 Convolution Filter는 각 Feature Map의 점을 중심으로 사물의 위치를 보정하여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Add, BatchNormalization\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 16\n",
    "K.clear_session()\n",
    "\n",
    "# 입력 Tensor : inputs = Input(shape=(128,128,3),name='images')\n",
    "# (None,None,3)으로 해야 이미지 크기에 무관하게 동작하지만, \n",
    "# 모델의 크기 변화를 확인하기 위해 (128,128,3)으로 고정시키고 진행하겠습니다.\n",
    "\n",
    "# 위 표에 맞는 base network를 구성해주세요\n",
    "\n",
    "# BASE NETWORK 구성하기\n",
    "base_network = Model(inputs, outputs,\n",
    "                     name='base_network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정을 메소드로 구성하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape=(None,None,3), num_units=16):\n",
    "    \"\"\"\n",
    "    input_shape : model의 input shape\n",
    "    num_units : 첫 번째 Block의 필터 갯수\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    \n",
    "    # fix me!\n",
    "    # 위 표에 맞는 base network를 구성해주세요\n",
    "\n",
    "    return Model(inputs, outputs, name='base_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network((128,128,3),16)\n",
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Box Head Network 구성하기\n",
    "---\n",
    "\n",
    "우리는 지정한 각 층에서 별도로 Classification과 Localization을 수행하는 Head network들을 둡니다. 하나의 Feature Map에서만 Head Network를 두는 것이 아니라, 여러 개의 Feature Map에 head Network를 둚으로써, 큰 이미지의 사물과 작은 이미지의 사물을 동시에 잘 잡아낼 수 있도록 설계하였습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/R7Zy1hc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub Network는 전혀 복잡한 것이 아닙니다. Sub Network는 Filter Size가 3x3인 Convolution Layer으로 구성되어 있는데, 위치 정보 Regressor는 출력 갯수가 4개(cx,cy,w,h)가 되도록, 라벨 정보 Classifier는 출력 갯수가 클래스 숫자만큼 되도록 구성하기만 하면 됩니다.\n",
    "\n",
    "위치 정보를 좀 더 구체적으로 살펴봅시다. 모델에서 추론하는 위치 정보는 `prior boxes`라 불리는 기준 박스 대비 사물이 얼마나 떨어져 있는지를 예측합니다. 즉 아래와 같이 구성됩니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/vMtbh6P.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위치 정보는 위와 같이 prior Box라 불리는 기준 박스 대비 실제 사물이 어디에 있나로 파악합니다. 그렇기 때문에 그 값의 범위는 음수에서 부터 양수의 크기를 가지고, 그 범위가 -1 ~ 1 사이에 들어올 수 있도록 Normalization해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 Multi-Head 모델은 3개의 Head Network, Head Network 별 2개의 출력값으로  총 6개의 출력값을 가지게 됩니다. 이러한 복수 개의 Output 형태는 이후에 후처리 과정이나 Loss를 계산할 때 좀 더 복잡해집니다. 이를 Reshape & Concat를 통해, 하나의 출력값으로 바꾸어 보도록 하겠습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/x9sKKyw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 + 1 # 0~9 + Background\n",
    "num_priors = 4\n",
    "\n",
    "# 본인이 설정한 norm layer의 이름을 가져와주세요\n",
    "source_layer_names = ['norm3_2','norm4_2','norm5_2']\n",
    "\n",
    "heads = []\n",
    "for idx, layer_name in enumerate(source_layer_names):\n",
    "    source_layer = base_network.get_layer(layer_name).output\n",
    "\n",
    "    # Classification\n",
    "    # fix me!\n",
    "\n",
    "    # Localization\n",
    "    # fix me!\n",
    "    \n",
    "    # Concatenate\n",
    "    # classification과 localization을 concate하고 heads에 넣어주세요\n",
    "    # fix me!\n",
    "    \n",
    "if len(heads) > 1:\n",
    "    predictions = Concatenate(axis=1, name='predictions')(heads)\n",
    "else:\n",
    "    predictions = K.identity(heads[0], name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 concat은 직관적입니다. Classifier와 Regressor을 열방향으로 Concat하고, Head끼리는 행방향으로 Concat하면 됩니다. 하지만 Reshape은 좀 더 복잡합니다. Reshape은 아래와 같은 순서로 배치됩니다. \n",
    "\n",
    "![Imgur](https://i.imgur.com/inoIiIq.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정들을 한 메소드로 구현하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_multibox_head(base_network, source_layer_names,\n",
    "                         num_priors=4, num_classes=10+1):\n",
    "    heads = []\n",
    "    for idx, layer_name in enumerate(source_layer_names):\n",
    "        source_layer = base_network.get_layer(layer_name).output\n",
    "        # 위의 인자들을 받아 heads를 구성하는 \n",
    "        # fix me!\n",
    "    \n",
    "    if len(heads) > 1:\n",
    "        predictions = Concatenate(axis=1, name='predictions')(heads)\n",
    "    else:\n",
    "        predictions = K.identity(heads[0],name='predictions')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tiny SSD Network 구성하기\n",
    "---\n",
    "\n",
    "위에서 구현한 메소드를 통해, 전체 모델을 구성해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network((128,128,3), num_units=16)\n",
    "predictions = attach_multibox_head(base_network,\n",
    "                                   ['norm3_2','norm4_2','norm5_2'],\n",
    "                                   num_classes=10+1)\n",
    "tiny_ssd = Model(base_network.input, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "해당 모델의 구조가 올바르게 잡혀 있는지를 Debug해보도록 하겠습니다. 우리가 관심있는 것은 출력값과 입력값이 어떻게 매칭되어 있는가 입니다. 이는 출력값과 입력값의 역전파를 통해 손쉽게 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset = DetectionDataset(data_type='validation')\n",
    "\n",
    "images, labels = validset[0:3]\n",
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Multi-box Head Network가 가지고 있는 Receptive Field의 영역을 알아보고자 합니다. 이는 Back Propagation을 통해 알 수 있습니다. Multi-Box Head들은 어느 Feature Map에서 가져왔느냐에 따라 그 Receptive Field가 다릅니다. 그것을 시각화해보도록 합시다. 이를 통해, 우리는 간접적으로 이후에 Prior box를 어떻게 설정해야 할지를 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "graph = sess.graph\n",
    "\n",
    "# 확인하고자 하는 Multi-box Head의 리스트들\n",
    "strides = [4,8,16]\n",
    "layer_names = ['loc_head0','loc_head1','loc_head2']\n",
    "\n",
    "inputs = model.inputs[0]\n",
    "for stride,layer_name in zip(strides,layer_names):\n",
    "    # 우리가 확인해보고 싶은 Multi-Box Head의 Layer\n",
    "    target_tensor = model.get_layer(layer_name).output\n",
    "    fig = plt.figure(figsize=(20,3))\n",
    "    plt.suptitle(f\"layer_name : {layer_name}\")\n",
    "    counter = 0 \n",
    "    for i in np.linspace(0,128//stride,4,dtype=np.int)[:-1]:\n",
    "        for j in np.linspace(0,128//stride,4,dtype=np.int)[:-1]:            \n",
    "            # Receptive Field을 역전파를 통해 계산하기\n",
    "            target_node = target_tensor[:,i,j,:]\n",
    "            grad = tf.gradients(target_node, inputs,\n",
    "                                grad_ys=tf.ones_like(target_node))[0]\n",
    "            output = sess.run(grad,feed_dict={ inputs:images })\n",
    "            receptive_field = (output>0).sum(axis=(0,-1))\n",
    "            \n",
    "            # Receptive_field의 범위 구하기\n",
    "            ymin,xmin = np.argwhere(receptive_field).min(axis=0)\n",
    "            ymax,xmax = np.argwhere(receptive_field).max(axis=0)\n",
    "            \n",
    "            # 결과를 사각형으로 그리기\n",
    "            blank = np.zeros_like(images[0],dtype=np.uint8)\n",
    "            visualized = cv2.rectangle(blank,\n",
    "                                       (xmin,ymin),(xmax,ymax),\n",
    "                                       (255,255,255),3)\n",
    "            \n",
    "            # 중심점을 찍기\n",
    "            center_x = stride*j+stride//2\n",
    "            center_y = stride*i+stride//2                \n",
    "            visualized[center_y-1:center_y+2,\n",
    "                       center_x-1:center_x+2,0] = 255\n",
    "            \n",
    "            # 이미지를 시각화\n",
    "            counter+=1\n",
    "            ax = fig.add_subplot(1, 9, counter)\n",
    "            ax.set_title([xmin,ymin,xmax,ymax])            \n",
    "            ax.imshow(visualized)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "⊙ Copyright(c) 2020 by PublicAI. All rights reserved <br>\n",
    "All pictures, codes, writings cannot be copied without permission. <br>\n",
    "Writen by PAI(info@publicai.co.kr) <br>\n",
    "last updated on 2020/01/4 <br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
